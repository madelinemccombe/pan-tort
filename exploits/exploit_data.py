# Copyright (c) 2018, Palo Alto Networks
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

# Author: Scott Shoaf <sshoaf@paloaltonetworks.com>

'''
Palo Alto Networks threat_data.py

Reads in a list of samples hashes, threatnames, or af_query as json

Output of malware verdict, file types,
malware family, and optionally signature coverage data.

Outputs are formatted for both bulk load into Elasticsearch and
readable 'pretty format' json

Outputs are stored in the out_estack and out_pretty directories

Before first use, create af_api.py with the Autofocus API key value
Then populate the hash_list and run the script

This software is provided without support, warranty, or guarantee.
Use at your own risk.
'''

import sys
import os
import json
import time
import csv
from datetime import datetime
import requests

# adding shared dir for imports
here = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.normpath(os.path.join(here, '../shared')))

# script to create or update the tagdata.json list from Autofocus
from gettagdata import tag_query

# local imports for static data input
import conf
from af_api import api_key
from filetypedata import filetypetags


def elk_index():
    '''
    set up elasticsearch bulk load index
    :param conf.elk_index_name: name of data index in elasticsearch
    :return: index tag to write as line in the output json file
    '''

    index_tag_full = {}
    index_tag_inner = {}
    index_tag_inner['_index'] = conf.elk_index_name
    index_tag_inner['_type'] = conf.elk_index_name
    index_tag_full['index'] = index_tag_inner

    return index_tag_full


def output_dir(dir_name):

    '''
    check for the output dirs and if exist=False then create them
    :param dir_name: directory name to be check and possibly created
    '''

    # check if the out_estack dir exists and if not then create it
    if os.path.isdir(dir_name) is False:
        os.mkdir(dir_name, mode=0o755)


def get_search_list():

    '''
    read in the list of elements from a text file
    :param filename: name of the search list file
    :return: return list of search values
    '''

    with open(conf.inputfile, 'r') as search_file:
        search_list = search_file.read().splitlines()

    return search_list


def clean_exploit_data():

    '''
    read csv vulnerability object file and create dict with CVE key
    :return: return cve dict
    '''

    # create cve_dict based on parse of vulnerability csv file
    cve_dict = {}

    # read in vulnerability csv file and parse
    # some CVE fields are also comma separated
    with open(conf.inputfile, newline='') as csvfile:
        reader = csv.DictReader(csvfile)

        for row in reader:
            # skip blank CVE records
            if row['CVE']:
                # break out multi-cve field so single cve value in dict
                if ',' in row['CVE']:
                    cve_many = row['CVE'].split(',')
                    for cve in cve_many:
                        cve_dict[cve] = {}
                        cve_dict[cve]['Threat Name'] = row['Threat Name']
                        cve_dict[cve]['Category'] = row['Category']
                        cve_dict[cve]['Severity'] = row['Severity']

                else:
                    cve_dict[row['CVE']] = {}
                    cve_dict[row['CVE']]['Threat Name'] = row['Threat Name']
                    cve_dict[row['CVE']]['Category'] = row['Category']
                    cve_dict[row['CVE']]['Severity'] = row['Severity']

    return cve_dict


def create_cve_list():

    '''
    read in autofocus cve tag data and get list of cve values
    :return: return list of cve values
    '''

    # create cve tags list based on parse of autofocus tag data
    cve_tag_list = []

    # for a current list, should run gettagdata.py periodically
    with open('tagdata.json', 'r') as tag_file:
        tag_dict = json.load(tag_file)

        for tag in tag_dict['_tags']:
            if 'CVE' in tag:
                if '_' not in tag:
                    cve_tag_list.append(tag_dict['_tags'][tag]['public_tag_name'])

    return cve_tag_list

def multi_query(searchlist):

    '''
    initial query into autofocus for a specific hash value
    :param hashvalue: hash for the search
    :return: autofocus response from initial query
    '''

    print('Initiating query to Autofocus')

    query = {"operator": "all",
             "children": [
                 {"field": "sample.malware", "operator": "is", "value": 1},
                 {"field": "sample.create_date", "operator": "is after",
                                      "value": ["2018-05-01T00:00:00", "2019-04-30T00:00:00"]},
                 {"field": "sample.tag", "operator": "is in the list",
                                      "value": searchlist}]}

    #print(query)

    search_values = {"apiKey": api_key,
                     "query": query,
                     "size": 4000,
                     "scope": "global",
                     "type": "scan",
                     "artifactSource": "af"
                     }


    headers = {"Content-Type": "application/json"}
    search_url = f'https://{conf.hostname}/api/v1.0/samples/search'

    try:
        search = requests.post(search_url, headers=headers, data=json.dumps(search_values))
        print('Search query posted to Autofocus')
        search.raise_for_status()
    except requests.exceptions.HTTPError:
        print(search)
        print(search.text)
        print('\nCorrect errors and rerun the application\n')
        sys.exit()

    search_dict = json.loads(search.text)

    return search_dict


def scantype_query_results(search_dict, start_time, query_tag, search, exploits):

    '''
    With type=scan each results post with the same cookie will return
    current set of hits
    This creates an extensible model for larger response sets > 4000
    Responses are returned in pages of 1000 entries
    Checks continue until search is complete and all pages of data returned
    :param search_dict: initial response including the cookie value
    :param start_time: when the script started - used to track run time
    :param query_tag: identifier for this script run used as estack tag
    :param search: for multi-page search to denote which 1000 block being used
    :return: autofocus search results dictionary or null if no hits
    '''

    autofocus_results = {}

    cookie = search_dict['af_cookie']
    print(f'Tracking cookie is {cookie}')
    print('Getting sample data...\n')

    search_progress = 'start'
    index = 1

    running_total = []
    running_length = []

    # looping across 1000 element input lists requires a file read if > 1 loops
    if search == 1:
        all_sample_dict = {}
        all_sample_dict['samples'] = []
    else:
        with open(f'{conf.out_pretty}/hash_data_pretty_{query_tag}_nosigs.json', 'r') as hash_file:
            all_sample_dict = json.load(hash_file)

    while search_progress != 'FIN':

        time.sleep(5)
        try:
            results_url = f'https://{conf.hostname}/api/v1.0/samples/results/' + cookie
            headers = {"Content-Type": "application/json"}
            results_values = {"apiKey": api_key}
            results = requests.post(results_url, headers=headers, data=json.dumps(results_values))
            results.raise_for_status()
        except requests.exceptions.HTTPError:
            print(results)
            print(results.text)
            print('\nCorrect errors and rerun the application\n')
            sys.exit()

        autofocus_results = results.json()


        if 'total' in autofocus_results:

            running_total.append(autofocus_results['total'])
            running_length.append(len(autofocus_results['hits']))

            if autofocus_results['total'] != 0:
                # parse data and output estack json elements
                # return is running dict of all samples for pretty json output
                all_sample_dict = parse_sample_data(autofocus_results, start_time, index, query_tag, all_sample_dict, search, exploits)
                with open(f'{conf.out_pretty}/hash_data_pretty_{query_tag}_nosigs.json', 'w') as hash_file:
                    hash_file.write(json.dumps(all_sample_dict, indent=2, sort_keys=False) + "\n")
                index += 1

                print(f'Results update for page {index}: {query_tag}\n')
                print(f"samples found so far: {autofocus_results['total']}")
                print(f"Search percent complete: {autofocus_results['af_complete_percentage']}%")
                print(f"samples processed in this batch: {len(autofocus_results['hits'])}")
                totalsamples = sum(running_length)
                print(f'total samples processed: {totalsamples}\n')
                minute_pts_rem = autofocus_results['bucket_info']['minute_points_remaining']
                daily_pts_rem = autofocus_results['bucket_info']['daily_points_remaining']
                print(f'AF quota update: {minute_pts_rem} minute points and {daily_pts_rem} daily points remaining')
                elapsedtime = datetime.now() - start_time
                print(f'Elasped run time is {elapsedtime}')
                print('=' * 80)


            if autofocus_results['af_in_progress'] is False :
                search_progress = 'FIN'
        else:
            print('Autofocus still queuing up the search...')

    print('\n')
    print('=' * 80)
    print('\n')
    print(f'sample processing complete for {query_tag}')
    print(f"total hits: {autofocus_results['total']}")
    totalsamples = sum(running_length)
    print(f'total samples processed: {totalsamples}')

    return autofocus_results



def parse_sample_data(autofocus_results, start_time, index, query_tag, hash_data_dict_pretty, search, exploits):

    '''
    parse the AF reponse and augment the data with file type, tag, malware
    then write 2 files: pretty json and estack for bulk load into elasticsearch
    :param autofocus_results: array of data from AF multi-query response
    :param start_time: time script started; used to track run time
    :param index: note which cycle through the search block for file w or a
    :param query_tag: identifier for this script run used as estack tag
    :param search: for multi-page search to denote which 1000 block being used
    :param hash_data_dict_pretty: master set of data to write out to json
    :return: update dictionary with sample data
    '''

    # mapping of tag # to text name
    malware_values = {'0': 'benign', '1': 'malware', '2': 'grayware', '3': 'phishing'}

    index_tag_full = elk_index()

    # used to have a full view of AF tag data for data augmentation
    # for a current list, should run gettagdata.py periodically
    with open('tagdata.json', 'r') as tag_file:
        tag_dict = json.load(tag_file)

    listsize = len(autofocus_results['hits'])

    # interate through AF results to create dict key/values for each sample hash
    for listpos in range(0, listsize):

        hash_data_dict = {}


        # AFoutput is json output converted to python dictionary
        hash_data_dict['sha256hash'] = autofocus_results['hits'][listpos]['_source']['sha256']
        hash_data_dict['create_date'] = autofocus_results['hits'][listpos]['_source']['create_date']
        hash_data_dict['query_tag'] = query_tag
        hash_data_dict['query_time'] = str(start_time)

        # initial AF query to get sample data include sha256 hash and WF verdict
        # sha256 is required for sig queries; does not support md5 or sha1
        verdict_num = autofocus_results['hits'][listpos]['_source']['malware']
        verdict_text = malware_values[str(verdict_num)]
        hash_data_dict['verdict'] = verdict_text

        if 'filetype' in autofocus_results['hits'][listpos]['_source']:
            filetype = autofocus_results['hits'][listpos]['_source']['filetype']
            hash_data_dict['filetype'] = filetype
            if filetype in filetypetags:
                hash_data_dict['filetype_group'] = filetypetags[filetype]
            else:
                hash_data_dict['filetype_group'] = 'NewTypeEh'
        else:
            hash_data_dict['filetype'] = 'Unknown'
            hash_data_dict['filetype_group'] = 'Unknown'

        if 'tag' in autofocus_results['hits'][listpos]['_source']:

            hash_data_dict['all_tags'] = autofocus_results['hits'][listpos]['_source']['tag']

            priority_tags_public = []
            priority_tags_name = []
            hash_data_dict['exploit_data'] = []

            for tag in hash_data_dict['all_tags']:

                if 'tag_class' in tag_dict['_tags'][tag]:

                    tag_class = tag_dict['_tags'][tag]['tag_class']
                    tag_name = tag_dict['_tags'][tag]['tag_name']
                    if tag_class in ('malware_family', 'campaign', 'actor', 'exploit'):
                        priority_tags_public.append(tag)
                        priority_tags_name.append(tag_name)

                    hash_data_dict['priority_tags_public'] = priority_tags_public
                    hash_data_dict['priority_tags_name'] = priority_tags_name

                if 'tag_groups' in tag_dict['_tags'][tag]:
                    taggroups = []
                    for group in tag_dict['_tags'][tag]['tag_groups']:
                        taggroups.append(group['tag_group_name'])

                    hash_data_dict['tag_groups'] = taggroups

                if 'CVE' in tag:

                    cve_value = tag.split('.')[1]
                    exploit_dict = {}
                    exploit_dict['cve_value'] = cve_value

                    if cve_value in exploits:
                        exploit_dict['threat name'] = exploits[cve_value]['Threat Name']
                        exploit_dict['category'] = exploits[cve_value]['Category']
                        exploit_dict['severity'] = exploits[cve_value]['Severity']

                    else:
                        exploit_dict['threat name'] = 'Unknown'
                        exploit_dict['category'] = 'Unknown'
                        exploit_dict['severity'] = 'Unknown'

                    hash_data_dict['exploit_data'].append(exploit_dict)

        # this creates a json format with first record as samples then appended json list entries
        # proper json format to read the file in during run to append with new data
        hash_data_dict_pretty['samples'].append(hash_data_dict)

        # Write dict contents to running file both estack and pretty json versions
        if index == 1 and listpos == 0 and search == 1:
            with open(f'{conf.out_estack}/hash_data_estack_{query_tag}_nosigs.json', 'w') as hash_file:
                hash_file.write(json.dumps(index_tag_full, indent=None, sort_keys=False) + "\n")
                hash_file.write(json.dumps(hash_data_dict, indent=None, sort_keys=False) + "\n")
        else:
            with open(f'{conf.out_estack}/hash_data_estack_{query_tag}_nosigs.json', 'a') as hash_file:
                hash_file.write(json.dumps(index_tag_full, indent=None, sort_keys=False) + "\n")
                hash_file.write(json.dumps(hash_data_dict, indent=None, sort_keys=False) + "\n")

        # only for hash searches
        #else:
        #    print('Ignoring unexpected hash found: ' + keyhash)

    return hash_data_dict_pretty


def main():

    '''search_data main module'''
    search_list_all = []

    # for longer lists may have to break list in 1000 size pieces
    # for autofocus type queries on do a single search
    numsearches = 1

    query_tag = input('Enter brief tag name for this data: ')
    start_time = datetime.now()
    listend = -1

    # refresh tag data list
    # the value sent to Autofocus should >> than current tag lists to set page count
    # as of 2019-05-16 list size is ~2900 items
    if conf.gettagdata == 'yes':
        tag_query(5000)

    # check for output dirs and created if needed
    output_dir(conf.out_estack)
    output_dir(conf.out_pretty)

    exploit_dict = clean_exploit_data()

    query_list = create_cve_list()

    listlength = len(query_list)
    numsearches = int(listlength / 1000) + 1

    for search in range(1, numsearches + 1):
        # submit bulk query for sample data to AF

        print(f'\nworking with search interval {search} of {numsearches}')
        liststart = listend + 1
        listend += 1000

        search_list = query_list[liststart:listend]
        print(f'query is sending {len(search_list)} items as search elements')

        searchrequest = multi_query(query_list)

        # get query results and parse output
        scantype_query_results(searchrequest, start_time, query_tag, search, exploit_dict)


if __name__ == '__main__':
    main()
